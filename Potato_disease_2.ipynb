{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af50fcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')  # will tell how GPU or CPU are in your pc\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48d226dc-dd9d-49ea-926f-dc8df61c8241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 355ms/step - accuracy: 0.5518 - loss: 1.2344 - val_accuracy: 0.7375 - val_loss: 0.5389\n",
      "Epoch 2/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 319ms/step - accuracy: 0.8146 - loss: 0.4612 - val_accuracy: 0.9250 - val_loss: 0.2395\n",
      "Epoch 3/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 301ms/step - accuracy: 0.8842 - loss: 0.3187 - val_accuracy: 0.8925 - val_loss: 0.2706\n",
      "Epoch 4/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 296ms/step - accuracy: 0.8974 - loss: 0.2779 - val_accuracy: 0.9050 - val_loss: 0.2195\n",
      "Epoch 5/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 295ms/step - accuracy: 0.9243 - loss: 0.1970 - val_accuracy: 0.9150 - val_loss: 0.2077\n",
      "Epoch 6/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 309ms/step - accuracy: 0.9219 - loss: 0.2091 - val_accuracy: 0.9400 - val_loss: 0.1470\n",
      "Epoch 7/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 304ms/step - accuracy: 0.9437 - loss: 0.1360 - val_accuracy: 0.9400 - val_loss: 0.1657\n",
      "Epoch 8/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 299ms/step - accuracy: 0.9330 - loss: 0.1831 - val_accuracy: 0.9275 - val_loss: 0.1458\n",
      "Epoch 9/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 295ms/step - accuracy: 0.9520 - loss: 0.1151 - val_accuracy: 0.9550 - val_loss: 0.0935\n",
      "Epoch 10/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 297ms/step - accuracy: 0.9592 - loss: 0.1140 - val_accuracy: 0.9075 - val_loss: 0.1969\n",
      "Epoch 11/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 307ms/step - accuracy: 0.9525 - loss: 0.1325 - val_accuracy: 0.9750 - val_loss: 0.0821\n",
      "Epoch 12/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 300ms/step - accuracy: 0.9569 - loss: 0.1033 - val_accuracy: 0.9025 - val_loss: 0.2098\n",
      "Epoch 13/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 307ms/step - accuracy: 0.9524 - loss: 0.1279 - val_accuracy: 0.9650 - val_loss: 0.0840\n",
      "Epoch 14/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 301ms/step - accuracy: 0.9585 - loss: 0.0972 - val_accuracy: 0.9525 - val_loss: 0.1172\n",
      "Epoch 15/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 306ms/step - accuracy: 0.9500 - loss: 0.1357 - val_accuracy: 0.9675 - val_loss: 0.0903\n",
      "Epoch 16/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 300ms/step - accuracy: 0.9533 - loss: 0.1015 - val_accuracy: 0.9675 - val_loss: 0.0897\n",
      "Epoch 17/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 299ms/step - accuracy: 0.9672 - loss: 0.0836 - val_accuracy: 0.9800 - val_loss: 0.0503\n",
      "Epoch 18/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 299ms/step - accuracy: 0.9749 - loss: 0.0742 - val_accuracy: 0.9875 - val_loss: 0.0662\n",
      "Epoch 19/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 296ms/step - accuracy: 0.9575 - loss: 0.1012 - val_accuracy: 0.9025 - val_loss: 0.2424\n",
      "Epoch 20/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 300ms/step - accuracy: 0.9618 - loss: 0.0935 - val_accuracy: 0.9125 - val_loss: 0.1977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f80767c8800>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Define the main directory for the dataset\n",
    "main_dir = 'Potato_disease/PlantVillage'\n",
    "\n",
    "# Image data generator for training data with augmentation and validation data with rescaling\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # 20% of the data will be used for validation\n",
    ")\n",
    "\n",
    "# Load training images from the main directory with the defined transformations\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    main_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Set as training data\n",
    ")\n",
    "\n",
    "# Load validation images from the main directory with only rescaling\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    main_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Set as validation data\n",
    ")\n",
    "\n",
    "\n",
    "# Create new model\n",
    "model = Sequential([\n",
    "    InputLayer(shape = (224,224,3)),\n",
    "    Conv2D(32, (3,3), activation = 'relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(32, (3,3), activation = 'relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "   \n",
    "\n",
    "    Conv2D(64, (3,3), activation = 'relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "  \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    \n",
    "    Dense(256, activation='relu'),\n",
    "\n",
    "    Dense(2, activation='softmax')  # Assuming you have 8 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',  # Use categorical cross-entropy for one-hot encoded labels\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae9926e8-5dbf-42a2-99b0-2cffa21f612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "def preprocess_image(img_path, target_size=(224, 224)):\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = img_array / 255.0  # Normalize to [0,1]  \n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0aa4f7fd-5243-41c5-ab37-af8b9ece5f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"
     ]
    }
   ],
   "source": [
    "path = 'Test_Images/Early_Blight_2.jpg'\n",
    "pred = model.predict(preprocess_image(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb6ac569-5a0c-46cf-8553-966ad2610775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9999952e-01, 4.3225685e-07]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e132bda-b871-425f-b5e3-253f09f64423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
